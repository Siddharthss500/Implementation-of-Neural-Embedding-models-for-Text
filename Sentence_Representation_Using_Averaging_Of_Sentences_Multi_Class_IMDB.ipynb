{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing sentence averaging for each sentence in a review in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/siddharth/gpu_codes/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Import all necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cPickle\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import re\n",
    "import csv\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializations\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto(log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all default values\n",
    "MAX_SENT_LENGTH = 100\n",
    "#MAX_SENTS = 15\n",
    "MAX_SENTS = 50\n",
    "MAX_NB_WORDS = 100000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean reviews\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"<br />\", \" \", string)\n",
    "    \n",
    "    string = re.sub(r\"[^A-Za-z0-9!.\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\" \\'t\", \"\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    \n",
    "    string = re.sub(r\"\\\\\", \"\", string)    \n",
    "    string = re.sub(r\"\\'\", \"\", string)    \n",
    "    string = re.sub(r\"\\\"\", \"\", string)    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dir = '/home/siddharth/TensorCode/My_Code/IMDB_Dataset/'\n",
    "folders = ['train','test']\n",
    "sub_folders = ['pos','neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total training examples are : 25000\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "id_count = 1\n",
    "Data = []\n",
    "# We observe that the file name is of the format \"id_rating.txt\"\n",
    "# Hence, we extract this information before reading the review presented in the file\n",
    "for i in sub_folders:\n",
    "    # Get the content from the folder\n",
    "    folder_name = new_dir + folders[0] + '/' + i + '/'\n",
    "    # Read every file in the folder and extract information of - id, rating, review\n",
    "    all_files = os.listdir(folder_name)\n",
    "    # Extract the information of the review and the rating\n",
    "    for j in all_files:\n",
    "        # Removing the extension \".txt\"\n",
    "        filetemp = j[:-4]\n",
    "        # Getting the rating of the review\n",
    "        rating = [filetemp[k+1:] for k in range(len(filetemp)) if filetemp[k] == '_']\n",
    "        filename = folder_name + j\n",
    "        # Read the review\n",
    "        with open(filename,'r') as infile:\n",
    "            review = infile.read()\n",
    "        #temp_review = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", review)\n",
    "        temp_review = review.lower()\n",
    "        temp_review2 = clean_str(temp_review)\n",
    "        fin_review = temp_review2.strip()\n",
    "        # Storing the data in record form\n",
    "        record = [str(id_count),int(rating[0]),fin_review]\n",
    "        Data.append(record)\n",
    "        id_count += 1\n",
    "print \"The total training examples are :\", (id_count-1)\n",
    "print \"All done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total test examples are : 25000\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "id_count = 1\n",
    "Test_Data = []\n",
    "# We observe that the file name is of the format \"id_rating.txt\"\n",
    "# Hence, we extract this information before reading the review presented in the file\n",
    "for i in sub_folders:\n",
    "    # Get the content from the folder\n",
    "    folder_name = new_dir + folders[1] + '/' + i + '/'\n",
    "    # Read every file in the folder and extract information of - id, rating, review\n",
    "    all_files = os.listdir(folder_name)\n",
    "    # Extract the information of the review and the rating\n",
    "    for j in all_files:\n",
    "        # Removing the extension \".txt\"\n",
    "        filetemp = j[:-4]\n",
    "        # Getting the rating of the review\n",
    "        rating = [filetemp[k+1:] for k in range(len(filetemp)) if filetemp[k] == '_']\n",
    "        filename = folder_name + j\n",
    "        # Read the review\n",
    "        with open(filename,'r') as infile:\n",
    "            review = infile.read()\n",
    "        #temp_review = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", review)\n",
    "        temp_review = review.lower()\n",
    "        temp_review2 = clean_str(temp_review)\n",
    "        fin_review = temp_review2.strip()\n",
    "        # Storing the data in record form\n",
    "        record = [str(id_count),int(rating[0]),fin_review]\n",
    "        Test_Data.append(record)\n",
    "        id_count += 1\n",
    "print \"The total test examples are :\", (id_count-1)\n",
    "print \"All done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done all the words present in the data corpus!\n",
      "There are a total of  108520 words in the data corpus.\n"
     ]
    }
   ],
   "source": [
    "# Get the dictionary of all words\n",
    "all_reviews = [Data[i][2] for i in range(len(Data))]\n",
    "findata, tempdata = [[] for j in range(2)]\n",
    "for i in all_reviews:\n",
    "    updatedata = i.split(' ')\n",
    "    for j in updatedata:\n",
    "        tempdata.append(j)    \n",
    "#Creating a unique list of words\n",
    "all_un_words = list(set(tempdata))\n",
    "print \"Done all the words present in the data corpus!\"\n",
    "print \"There are a total of \", len(all_un_words), \"words in the data corpus.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for all the words present in the data corpus!\n",
      "There are a total of  107367 words in the data corpus.\n"
     ]
    }
   ],
   "source": [
    "# Get the dictionary of all words\n",
    "all_test_reviews = [Test_Data[i][2] for i in range(len(Test_Data))]\n",
    "findata, tempdata = [[] for j in range(2)]\n",
    "for i in all_test_reviews:\n",
    "    updatedata = i.split(' ')\n",
    "    for j in updatedata:\n",
    "        tempdata.append(j)    \n",
    "#Creating a unique list of words\n",
    "all_test_un_words = list(set(tempdata))\n",
    "print \"Done for all the words present in the data corpus!\"\n",
    "print \"There are a total of \", len(all_test_un_words), \"words in the data corpus.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "Counter({1: 5100, 10: 4732, 8: 3009, 4: 2696, 7: 2496, 3: 2420, 2: 2284, 9: 2263})\n"
     ]
    }
   ],
   "source": [
    "# Get all the labels\n",
    "all_labels = [Data[i][1] for i in range(len(Data))]\n",
    "\n",
    "print len(all_labels)\n",
    "print Counter(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "Counter({1: 5022, 10: 4999, 8: 2850, 4: 2635, 3: 2541, 9: 2344, 7: 2307, 2: 2302})\n"
     ]
    }
   ],
   "source": [
    "# Get all the labels\n",
    "all_test_labels = [Test_Data[i][1] for i in range(len(Test_Data))]\n",
    "\n",
    "print len(all_test_labels)\n",
    "print Counter(all_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "\n",
    "reviews, labels, texts, test_reviews, test_labels, test_texts = [[] for i in range(6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddharth/gpu_codes/local/lib/python2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file /usr/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"html5lib\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "# For training examples\n",
    "# Clean and tokenize each review. Get each review as a list of sentences.\n",
    "for idx in range(len(all_reviews)):\n",
    "    text = BeautifulSoup(all_reviews[idx])\n",
    "    text = clean_str(text.get_text().encode('ascii','ignore'))\n",
    "    texts.append(text)\n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    reviews.append(sentences)\n",
    "    \n",
    "    #labels.append(all_labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test examples\n",
    "# Clean and tokenize each review. Get each review as a list of sentences.\n",
    "for idx in range(len(all_test_reviews)):\n",
    "    text = BeautifulSoup(all_test_reviews[idx])\n",
    "    text = clean_str(text.get_text().encode('ascii','ignore'))\n",
    "    test_texts.append(text)\n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    test_reviews.append(sentences)\n",
    "    \n",
    "    #test_labels.append(all_test_labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done getting the labels in the right format!\n"
     ]
    }
   ],
   "source": [
    "# Creating a 25000 x 8 matrix using one-hot coding\n",
    "labels = np.zeros((len(texts), 8), dtype='float32')\n",
    "\n",
    "for i in range(len(all_labels)):\n",
    "    temp_labels = np.zeros(8)\n",
    "    if all_labels[i] <5:\n",
    "        index = all_labels[i] - 1\n",
    "    else:\n",
    "        index = all_labels[i] - 3\n",
    "    temp_labels[index] = 1\n",
    "    labels[i] = temp_labels\n",
    "\n",
    "print \"Done getting the labels in the right format!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done getting the labels in the right format!\n"
     ]
    }
   ],
   "source": [
    "# Creating a 25000 x 8 matrix using one-hot coding\n",
    "test_labels = np.zeros((len(test_texts), 8), dtype='float32')\n",
    "\n",
    "for i in range(len(all_test_labels)):\n",
    "    temp_labels = np.zeros(8)\n",
    "    if all_test_labels[i] <5:\n",
    "        index = all_test_labels[i] - 1\n",
    "    else:\n",
    "        index = all_test_labels[i] - 3\n",
    "    temp_labels[index] = 1\n",
    "    test_labels[i] = temp_labels\n",
    "\n",
    "print \"Done getting the labels in the right format!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Basic checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the first two sequences of this movie set up the two conflicts the thematic conflict between the soldier todd and his suppressed humanity and the physical conflict between todd and his bio engineered replacement. both sequences are quite gripping in different ways. peoples screenplay falters somewhat by resolving the first of these arcs half way through the movie which means the second half is little more than a straightforward action romp. nonetheless kudos to the makers for creating an genre action piece with heart and even a bit of soul and especially to kurt russell who conveys much with very little. not a great film but one worth seeing.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i guess if they are not brother this film will became very common. how long were they can keep this if we were part what should they do so natural feelings so plain and barren words.but i almost cried last night blood relationship brotherhood love knot film.in another word the elder brother is very cute.if they are not brothers they wont have so many forbidden factors from the family society friends even hearts of their own at the very beginning.the elder brother is doubtful of whether he is coming out or not at the beginning .maybe the little brother being so long time with his brother and even cant got any praise from his father this made him very upset and even sad maybe this is a key blasting fuse let him feel there were no one in the world loving him except his beloved brother. and i want to say this is a so human natural feeling there is nothing to be shamed you may fell in love your mother brother sister.just a frail heart looking for backbone to rely on'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum number of sentences in a review are : 282\n",
      "The minimum number of sentences in a review are : 1\n"
     ]
    }
   ],
   "source": [
    "# Calculating the maximum number of sentences a review has\n",
    "MAX_SENTS_1 = 0\n",
    "MIN_SENTS = 100\n",
    "for review in reviews:\n",
    "    temp = len(review)\n",
    "    if temp > MAX_SENTS_1:\n",
    "        MAX_SENTS_1 = temp\n",
    "    if temp < MIN_SENTS:\n",
    "        MIN_SENTS = temp\n",
    "print \"The maximum number of sentences in a review are :\", MAX_SENTS_1\n",
    "print \"The minimum number of sentences in a review are :\", MIN_SENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "#tokenizer.fit_on_texts(texts)\n",
    "tokenizer.fit_on_texts(texts + test_texts)\n",
    "\n",
    "data = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "test_data = np.zeros((len(test_texts), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29845"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['fawn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 102390 unique tokens.\n",
      "('Shape of data tensor:', (25000, 50, 100))\n",
      "('Shape of label tensor:', (25000, 8))\n"
     ]
    }
   ],
   "source": [
    "# For training examples\n",
    "# Create our input tensors - Replace zeros with the IDs of each word as created by the tokenizer\n",
    "for i, sentences in enumerate(reviews):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j< MAX_SENTS:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k=0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                if k<MAX_SENT_LENGTH and tokenizer.word_index[word]<MAX_NB_WORDS:\n",
    "                    data[i,j,k] = tokenizer.word_index[word]\n",
    "                    k=k+1                    \n",
    "                    \n",
    "word_index = tokenizer.word_index\n",
    "print('Total %s unique tokens.' % len(word_index))\n",
    "\n",
    "#labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 102390 unique tokens.\n",
      "('Shape of data tensor:', (25000, 50, 100))\n",
      "('Shape of label tensor:', (25000, 8))\n"
     ]
    }
   ],
   "source": [
    "# For test data\n",
    "# Create our input tensors - Replace zeros with the IDs of each word as created by the tokenizer\n",
    "for i, sentences in enumerate(test_reviews):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j< MAX_SENTS:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k=0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                if k<MAX_SENT_LENGTH and tokenizer.word_index[word]<MAX_NB_WORDS:\n",
    "                    test_data[i,j,k] = tokenizer.word_index[word]\n",
    "                    k=k+1                    \n",
    "                    \n",
    "test_word_index = tokenizer.word_index\n",
    "print('Total %s unique tokens.' % len(test_word_index))\n",
    "\n",
    "#test_labels = to_categorical(np.asarray(test_labels))\n",
    "print('Shape of data tensor:', test_data.shape)\n",
    "print('Shape of label tensor:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive and negative reviews in training and validation set\n",
      "[4076. 1854. 1928. 2143. 1991. 2405. 1821. 3782.]\n",
      "[1024.  430.  492.  553.  505.  604.  442.  950.]\n",
      "Number of positive and negative reviews in test set\n",
      "[5022. 2302. 2541. 2635. 2307. 2850. 2344. 4999.]\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the data and split the data into train and validation as 0.8 and 0.2\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "print('Number of positive and negative reviews in training and validation set')\n",
    "print (y_train.sum(axis=0))\n",
    "print (y_val.sum(axis=0))\n",
    "\n",
    "# Store the test data\n",
    "\n",
    "x_test = test_data[:]\n",
    "y_test = test_labels[:]\n",
    "\n",
    "print('Number of positive and negative reviews in test set')\n",
    "print (y_test.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Get the word representations from the file\n",
    "GLOVE_DIR = \"/home/siddharth/TensorCode/word2vecs/\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.300d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras layer with performs sentence averaging\n",
    "class SentAvg(Layer):\n",
    "    \n",
    "    # This function is used to set the basic initializations \n",
    "    def __init__(self, **kwargs):\n",
    "        super(SentAvg, self).__init__(**kwargs)\n",
    "        \n",
    "    # This function is to define the weights\n",
    "    def build(self, input_shape):\n",
    "        super(SentAvg, self).build(input_shape)\n",
    "    \n",
    "    # This function is where our main task is implemented. In our case, it is the sentence averaging.\n",
    "    def call(self, x, mask=None):\n",
    "        return K.mean(x, axis=1)\n",
    "    \n",
    "    # This function sets the shape of the output\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras layer with builds the attention layer\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializations.get('normal')\n",
    "        #self.input_spec = [InputSpec(ndim=3)]\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        #self.W = self.init((input_shape[-1],1))\n",
    "        self.W = self.init((input_shape[-1],))\n",
    "        #self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        #eij = K.tanh(K.dot(x, self.W))\n",
    "        eij = K.tanh(K.squeeze(K.dot(x, K.expand_dims(self.W)), axis=-1))\n",
    "        \n",
    "        ai = K.exp(eij)\n",
    "        #weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "        weights = ai/K.expand_dims(K.sum(ai, axis=1),1)\n",
    "        \n",
    "        weighted_input = x*K.expand_dims(weights,2)\n",
    "        #weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        #return weighted_input.sum(axis=1)\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building Hierachical Attention network\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "# Embedding layer in keras        \n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating layers to average all words in a sentence\n",
    "# First layer performs averaging at a word level for each sentence present in a review\n",
    "# Second layer sends each review to the first layer and then is passed through an attention layer\n",
    "\n",
    "# Input layer for sentences in a review\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "# Get embedding for the sentences in a review\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "# Average all words in each sentence present in a review\n",
    "sent_avg = SentAvg()(embedded_sequences)\n",
    "# A first layer is formed for a two layer MLP\n",
    "sentEncoder = Model(sentence_input, sent_avg)\n",
    "\n",
    "# Input layer for reviews\n",
    "review_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH), dtype='int32')\n",
    "# Get the sentences averaged for each review by calling the previous layer\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "# Build the attention layer for sentences present in a review\n",
    "l_att_sent = AttLayer()(review_encoder)\n",
    "# Build an activation function - softmax on our attention layer\n",
    "preds = Dense(8, activation='softmax')(l_att_sent)\n",
    "# Finish the final layer\n",
    "model = Model(review_input, preds)\n",
    "# Define the loss function, optimization technique and metric\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Sum_1:0' shape=(?, 300) dtype=float32>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_att_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Softmax:0' shape=(?, 8) dtype=float32>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the model with Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - Hierachical attention network\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 50, 100)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_1 (TimeDistribut (None, 50, 300)       30717300    input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "attlayer_1 (AttLayer)            (None, 300)           300         timedistributed_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 8)             2408        attlayer_1[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 30,720,008\n",
      "Trainable params: 30,720,008\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/50\n",
      "20000/20000 [==============================] - 21s - loss: 2.0362 - acc: 0.2025 - val_loss: 2.0410 - val_acc: 0.2056\n",
      "Epoch 2/50\n",
      "20000/20000 [==============================] - 19s - loss: 2.0177 - acc: 0.2217 - val_loss: 2.0170 - val_acc: 0.2050\n",
      "Epoch 3/50\n",
      "20000/20000 [==============================] - 20s - loss: 1.9944 - acc: 0.2472 - val_loss: 1.9880 - val_acc: 0.2976\n",
      "Epoch 4/50\n",
      "20000/20000 [==============================] - 19s - loss: 1.9582 - acc: 0.2880 - val_loss: 1.9459 - val_acc: 0.2602\n",
      "Epoch 5/50\n",
      "20000/20000 [==============================] - 20s - loss: 1.9148 - acc: 0.3151 - val_loss: 1.8998 - val_acc: 0.3046\n",
      "Epoch 6/50\n",
      "20000/20000 [==============================] - 20s - loss: 1.8653 - acc: 0.3296 - val_loss: 1.8436 - val_acc: 0.3418\n",
      "Epoch 7/50\n",
      "20000/20000 [==============================] - 20s - loss: 1.8144 - acc: 0.3495 - val_loss: 1.8039 - val_acc: 0.3440\n",
      "Epoch 8/50\n",
      "20000/20000 [==============================] - 20s - loss: 1.7652 - acc: 0.3625 - val_loss: 1.7736 - val_acc: 0.3466\n",
      "Epoch 9/50\n",
      "20000/20000 [==============================] - 20s - loss: 1.7186 - acc: 0.3715 - val_loss: 1.7139 - val_acc: 0.3594\n",
      "Epoch 10/50\n",
      "20000/20000 [==============================] - 21s - loss: 1.6739 - acc: 0.3863 - val_loss: 1.6890 - val_acc: 0.3736\n",
      "Epoch 11/50\n",
      "20000/20000 [==============================] - 19s - loss: 1.6332 - acc: 0.3951 - val_loss: 1.6527 - val_acc: 0.3856\n",
      "Epoch 12/50\n",
      "20000/20000 [==============================] - 20s - loss: 1.5966 - acc: 0.4048 - val_loss: 1.6252 - val_acc: 0.3766\n",
      "Epoch 13/50\n",
      "20000/20000 [==============================] - 19s - loss: 1.5644 - acc: 0.4167 - val_loss: 1.6032 - val_acc: 0.3936\n",
      "Epoch 14/50\n",
      "20000/20000 [==============================] - 20s - loss: 1.5341 - acc: 0.4228 - val_loss: 1.5788 - val_acc: 0.3940\n",
      "Epoch 15/50\n",
      "20000/20000 [==============================] - 20s - loss: 1.5079 - acc: 0.4328 - val_loss: 1.5454 - val_acc: 0.4032\n",
      "Epoch 16/50\n",
      "20000/20000 [==============================] - 20s - loss: 1.4827 - acc: 0.4410 - val_loss: 1.5503 - val_acc: 0.4072\n",
      "Epoch 17/50\n",
      "20000/20000 [==============================] - 20s - loss: 1.4603 - acc: 0.4451 - val_loss: 1.5244 - val_acc: 0.4140\n",
      "Epoch 18/50\n",
      "20000/20000 [==============================] - 22s - loss: 1.4393 - acc: 0.4568 - val_loss: 1.5128 - val_acc: 0.4262\n",
      "Epoch 19/50\n",
      "20000/20000 [==============================] - 22s - loss: 1.4190 - acc: 0.4625 - val_loss: 1.4904 - val_acc: 0.4310.46\n",
      "Epoch 20/50\n",
      "20000/20000 [==============================] - 22s - loss: 1.4011 - acc: 0.4695 - val_loss: 1.4867 - val_acc: 0.4314\n",
      "Epoch 21/50\n",
      "20000/20000 [==============================] - 21s - loss: 1.3837 - acc: 0.4761 - val_loss: 1.4821 - val_acc: 0.4306\n",
      "Epoch 22/50\n",
      "20000/20000 [==============================] - 21s - loss: 1.3679 - acc: 0.4805 - val_loss: 1.4644 - val_acc: 0.4298\n",
      "Epoch 23/50\n",
      "20000/20000 [==============================] - 21s - loss: 1.3507 - acc: 0.4886 - val_loss: 1.4616 - val_acc: 0.4274\n",
      "Epoch 24/50\n",
      "20000/20000 [==============================] - 22s - loss: 1.3370 - acc: 0.4930 - val_loss: 1.4530 - val_acc: 0.4270\n",
      "Epoch 25/50\n",
      "20000/20000 [==============================] - 23s - loss: 1.3213 - acc: 0.5009 - val_loss: 1.4482 - val_acc: 0.4368\n",
      "Epoch 26/50\n",
      "20000/20000 [==============================] - 21s - loss: 1.3082 - acc: 0.5095 - val_loss: 1.4458 - val_acc: 0.4456\n",
      "Epoch 27/50\n",
      "20000/20000 [==============================] - 22s - loss: 1.2921 - acc: 0.5123 - val_loss: 1.4507 - val_acc: 0.4372\n",
      "Epoch 28/50\n",
      "20000/20000 [==============================] - 22s - loss: 1.2786 - acc: 0.5211 - val_loss: 1.4368 - val_acc: 0.4330\n",
      "Epoch 29/50\n",
      "20000/20000 [==============================] - 21s - loss: 1.2659 - acc: 0.5222 - val_loss: 1.4261 - val_acc: 0.4440\n",
      "Epoch 30/50\n",
      "20000/20000 [==============================] - 22s - loss: 1.2508 - acc: 0.5333 - val_loss: 1.4231 - val_acc: 0.4416\n",
      "Epoch 31/50\n",
      "20000/20000 [==============================] - 22s - loss: 1.2369 - acc: 0.5419 - val_loss: 1.4183 - val_acc: 0.4460\n",
      "Epoch 32/50\n",
      "20000/20000 [==============================] - 21s - loss: 1.2229 - acc: 0.5501 - val_loss: 1.4260 - val_acc: 0.4438\n",
      "Epoch 33/50\n",
      "20000/20000 [==============================] - 21s - loss: 1.2095 - acc: 0.5541 - val_loss: 1.4220 - val_acc: 0.4488\n",
      "Epoch 34/50\n",
      "20000/20000 [==============================] - 21s - loss: 1.1964 - acc: 0.5594 - val_loss: 1.4144 - val_acc: 0.4462\n",
      "Epoch 35/50\n",
      "20000/20000 [==============================] - 20s - loss: 1.1819 - acc: 0.5696 - val_loss: 1.4131 - val_acc: 0.4488\n",
      "Epoch 36/50\n",
      "20000/20000 [==============================] - 20s - loss: 1.1686 - acc: 0.5763 - val_loss: 1.4079 - val_acc: 0.4500\n",
      "Epoch 37/50\n",
      "20000/20000 [==============================] - 19s - loss: 1.1553 - acc: 0.5845 - val_loss: 1.4087 - val_acc: 0.4476\n",
      "Epoch 38/50\n",
      "20000/20000 [==============================] - 19s - loss: 1.1406 - acc: 0.5943 - val_loss: 1.4055 - val_acc: 0.4488\n",
      "Epoch 39/50\n",
      "20000/20000 [==============================] - 19s - loss: 1.1275 - acc: 0.5976 - val_loss: 1.4105 - val_acc: 0.4452\n",
      "Epoch 40/50\n",
      "20000/20000 [==============================] - 19s - loss: 1.1140 - acc: 0.6074 - val_loss: 1.4044 - val_acc: 0.4514\n",
      "Epoch 41/50\n",
      "20000/20000 [==============================] - 19s - loss: 1.1002 - acc: 0.6122 - val_loss: 1.4075 - val_acc: 0.4506\n",
      "Epoch 42/50\n",
      "20000/20000 [==============================] - 19s - loss: 1.0862 - acc: 0.6196 - val_loss: 1.4053 - val_acc: 0.4490\n",
      "Epoch 43/50\n",
      "20000/20000 [==============================] - 17s - loss: 1.0719 - acc: 0.6280 - val_loss: 1.4065 - val_acc: 0.4480\n",
      "Epoch 44/50\n",
      "20000/20000 [==============================] - 16s - loss: 1.0577 - acc: 0.6372 - val_loss: 1.4167 - val_acc: 0.4412\n",
      "Epoch 45/50\n",
      "20000/20000 [==============================] - 19s - loss: 1.0444 - acc: 0.6420 - val_loss: 1.4082 - val_acc: 0.4542\n",
      "Epoch 46/50\n",
      "20000/20000 [==============================] - 20s - loss: 1.0304 - acc: 0.6522 - val_loss: 1.4017 - val_acc: 0.4532\n",
      "Epoch 47/50\n",
      "20000/20000 [==============================] - 20s - loss: 1.0151 - acc: 0.6561 - val_loss: 1.4146 - val_acc: 0.4478\n",
      "Epoch 48/50\n",
      "20000/20000 [==============================] - 21s - loss: 1.0022 - acc: 0.6636 - val_loss: 1.3997 - val_acc: 0.4546\n",
      "Epoch 49/50\n",
      "20000/20000 [==============================] - 20s - loss: 0.9873 - acc: 0.6740 - val_loss: 1.4015 - val_acc: 0.4554\n",
      "Epoch 50/50\n",
      "20000/20000 [==============================] - 19s - loss: 0.9742 - acc: 0.6779 - val_loss: 1.4029 - val_acc: 0.4570\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f11e8de5690>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"model fitting - Hierachical attention network\")\n",
    "print (model.summary())\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          nb_epoch=50, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing on test dataset with MAX_SENTS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24992/25000 [============================>.] - ETA: 0s\n",
      "acc: 44.30%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test,y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24950/25000 [============================>.] - ETA: 0s0.44296\n",
      "[1.4363648743629456, 0.44295999947190284]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "Y_pred = np.argmax(y_pred,axis=1)\n",
    "Y_test = np.argmax(y_test,axis=1)\n",
    "accuracy = (len(Y_test) - np.count_nonzero(Y_pred - Y_test) + 0.0)/len(Y_test)\n",
    "score = model.evaluate(x_test, y_test, batch_size=50, verbose=1)\n",
    "\n",
    "print (accuracy)\n",
    "print (score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing on test dataset with MAX_SENTS = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24864/25000 [============================>.] - ETA: 0s\n",
      "acc: 41.12%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test,y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24900/25000 [============================>.] - ETA: 0s0.41116\n",
      "[1.6185614216327666, 0.41115999895334243]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "Y_pred = np.argmax(y_pred,axis=1)\n",
    "Y_test = np.argmax(y_test,axis=1)\n",
    "accuracy = (len(Y_test) - np.count_nonzero(Y_pred - Y_test) + 0.0)/len(Y_test)\n",
    "score = model.evaluate(x_test, y_test, batch_size=50, verbose=1)\n",
    "\n",
    "print (accuracy)\n",
    "print (score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
